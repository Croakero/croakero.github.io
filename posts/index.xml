<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>所有文章 - croakero</title>
        <link>http://croakero.github.io/posts/</link>
        <description>所有文章 | croakero</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Wed, 17 Jul 2024 09:43:24 &#43;0800</lastBuildDate><atom:link href="http://croakero.github.io/posts/" rel="self" type="application/rss+xml" /><item>
    <title>使用 LLaMA Factory 训练模型指北（二、从 Webui 入手）</title>
    <link>http://croakero.github.io/posts/%E4%BD%BF%E7%94%A8-llama-factory-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%8C%87%E5%8C%97%E4%BA%8C%E4%BB%8E-webui-%E5%85%A5%E6%89%8B/</link>
    <pubDate>Wed, 17 Jul 2024 09:43:24 &#43;0800</pubDate>
    <author>croakero</author>
    <guid>http://croakero.github.io/posts/%E4%BD%BF%E7%94%A8-llama-factory-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%8C%87%E5%8C%97%E4%BA%8C%E4%BB%8E-webui-%E5%85%A5%E6%89%8B/</guid>
    <description><![CDATA[<p>在上篇内容中我们讲解了将 LLaMA-Factory 部署到远程（或本地）服务器的方法，并且能够正常启动 webui ，使用了 tmux 复用终端，解决ssh连接的问题，现在我们直接在浏览器输入对应地址，就可以看到清爽的界面，下一步就是解释 webui 上向我们展示的内容。</p>
<h2 id="1概览">1.概览</h2>
<p>webui 差不多长这个样子
<figure><a class="lightgallery" href="../images/llama7.png" title="webui" data-thumbnail="../images/llama7.png" data-sub-html="<h2>不要被它吓到了</h2><p>webui</p>">
        
    </a><figcaption class="image-caption">不要被它吓到了</figcaption>
    </figure></p>
<table>
<thead>
<tr>
<th>区域</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>语言</td>
<td>顾名思义，可以选择对应的语言，有中文（zh）英文（en）和俄文（ru）</td>
</tr>
<tr>
<td>模型名称</td>
<td>可选的有非常多来自huggingface社区的支持，注意，这些模型并不保存在你的本地，而是在<a href="https://huggingface.co/" target="_blank" rel="noopener noreffer ">https://huggingface.co/</a>这个网站上面，在使用的时候会下载到本地，如果你采取这种方式，遵循<a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener noreffer "> LLaMA-Factory 项目地址</a>中的方法就好，如果你下载到本地，由于不明确路径是什么，我直接填写了绝对路径上去，实测是可以运行的，也可以填写相对路径，相对路径应该是相对于LLaMA-Factory目录内部的路径</td>
</tr>
<tr>
<td>模型路径</td>
<td>本地文件，填写本地路径；如果直接用 huggingface 上的模型，就填写模型路径，在选择模型名称时会自动填写</td>
</tr>
</tbody>
</table>
<p>第一行就是这样，在上面的图片中，我采用的是 LLaMA3B-Chinese-Chat 这个模型，保存在本地，使用绝对路径。</p>
<h2 id="2微调方法和检查点路径">2.微调方法和检查点路径</h2>
<p>微调方法可选的有三种类，见下面的表格，该表中的内容在命令中反映为<code>--finetuning_type</code></p>
<table>
<thead>
<tr>
<th>微调方法</th>
<th>介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td>full</td>
<td>全量微调，把整个预训练模型的每一个层，所有的模型参数都进行调整，来适应我们需要的下游任务， GPU 计算资源占用相当高，是一种比较早期的模型训练方法，后来的一些研究和论文研究了其他微调的策略，只对部分参数进行微调，有一些能在大大减少 GPU 占用的情况下达到相似的效果</td>
</tr>
<tr>
<td>freeze</td>
<td>冻结方法，将整个预训练模型的大多数层冻结，只调整部分层的参数，一般是后面的层，因为前面的层更倾向于处理输入，后面的层更贴近输出，大模型在前面的层已经很好学到了语言能力，主要是调整其组织语言的方式，由于对调整参数较少， GPU 占用相应地比全量微调低一些</td>
</tr>
<tr>
<td>LoRA</td>
<td>低秩适应，一种又新又好的方法，<a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="noopener noreffer ">原论文链接</a>，有洋文阅读问题的话，这是一个还行的<a href="https://www.cnblogs.com/xiximayou/p/17283052.html" target="_blank" rel="noopener noreffer ">论文翻译</a>，它基于一个“模型的适应过程中权重的变化具有较低的内在秩”的假设，虽然一般来说层的权重矩阵具有全秩，但是当适应一个特定的任务的时候，这个语言模型的矩阵维度会降低，按我的理解就是矩阵从它的向量空间映射到它的子空间（也就是从原本任务映射到一个子下游任务），其中有一些维度的内容不在子空间中起效果，比如一个描述三维空间的矩阵投影到二维，有一个维度被舍弃了，这个多余的维度就是不在子任务中的内容。</td>
</tr>
</tbody>
</table>
<p>下面是不同微调方法所需的 GPU 显存，其中 QLoRA 是损失精度，量化后的 LoRA 方法</p>
<p><figure><a class="lightgallery" href="../images/llama8.png" title="训练要求" data-thumbnail="../images/llama8.png" data-sub-html="<h2>LLAMA-Factory 给出的不同微调方法所需 GPU 显存</h2><p>训练要求</p>">
        
    </a><figcaption class="image-caption">LLAMA-Factory 给出的不同微调方法所需 GPU 显存</figcaption>
    </figure></p>
<p>检查点路径一栏给出了检查点保存的位置，在命令中反映为<code>--adapter_name_or_path</code>，每次训练结束之后，训练的“新”内容会存储在这个位置，需要把它和原模型合并，才能输出为一个新的总模型的形式，在你没有合并模型的时候，选择对应的检查点路径可以非常方便地加载“总”模型，而不用你再手动每次都合并后再测试。合并模型在 Export 一栏，选择对应检查点后直接导出就好。</p>
<h2 id="3train-训练一栏">3.Train 训练一栏</h2>
<table>
<thead>
<tr>
<th>训练阶段可选项</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>Supervised Fine-Tuning（监督微调，SFT）</td>
<td>使用一个已经训练好的模型，也就是预训练模型作为初始状态，然后使用目标任务的训练集对模型进行微调，避免了从头开始训练模型，一般只修改模型中的某些层。</td>
</tr>
<tr>
<td>Reward Modeling（奖励建模）</td>
<td>当模型产生一个输出时，人类会对它的输出进行评估，告诉它什么是好的什么是不好的，让大模型来向着好的方向努力，避免产生坏的结果的的训练方法。</td>
</tr>
<tr>
<td>PPO</td>
<td>是 OpenAI 在 2017 年提出的一种强化学习算法，是基于策略优化的算法，用于训练能够最大化累积奖励的智能体。 PPO 算法通过在每次更新时限制新策略与旧策略之间的差异，从而更稳定地更新策略参数。这种方法有助于避免训练过程中出现的不稳定性和剧烈波动，使得算法更容易收敛并学习到更好的策略。<a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener noreffer ">论文链接</a></td>
</tr>
<tr>
<td>DPO</td>
<td>一种稳定的、性能和计算成本轻量级的强化学习算法。通过利用奖励函数与最优策略之间的映射关系，证明这个受限的奖励最大化问题可以通过单阶段的策略训练来精确优化，本质上是在人类偏好数据上解决一个分类问题。即一种稳定低成本的 PPO ,<a href="https://arxiv.org/abs/2305.18290" target="_blank" rel="noopener noreffer ">论文链接</a></td>
</tr>
<tr>
<td>KTO</td>
<td>KTO不需要偏好数据，可以直接利用二元信号标记的数据来训练算法，对于负样本更加敏感。实验表明，KTO算法在一定参数范围内能够超过DPO算法，并且KTO可以处理数据正负样本不平衡的情况。<a href="https://arxiv.org/abs/2402.01306" target="_blank" rel="noopener noreffer ">论文链接</a>还有不错的<a href="https://zhuanlan.zhihu.com/p/696331582" target="_blank" rel="noopener noreffer ">中文讲解</a></td>
</tr>
</tbody>
</table>
<p>数据路径默认为 data ，就是 LLaMA-Factory 文件夹下（后面以相对路径 LLaMA-Factory 目录下表示，不再赘述）</p>
<p>数据集可选，一些 demo 保存在 data 文件夹下，可以直接测试。</p>
<p><strong>注意：</strong> 如果你自己新建数据集，一定要把你的数据集放在 data 目录下，并且在 dataset_info.json中添加，<strong>并且要先看 <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md" target="_blank" rel="noopener noreffer ">README_zh.md</a> 下的内容</strong></p>
<p>比如我新建了一个叫做 &ldquo;0_600.json&rdquo; 的数据集，描述为stack_demo，那么应该在 dataset_info.json 中是这样：
<figure><a class="lightgallery" href="../images/llama9.png" title="dataset_info" data-thumbnail="../images/llama9.png" data-sub-html="<h2>dataset_info.json 中</h2><p>dataset_info</p>">
        
    </a><figcaption class="image-caption">dataset_info.json 中</figcaption>
    </figure>
在webui中长这样：
<figure><a class="lightgallery" href="../images/llama10.png" title="webui" data-thumbnail="../images/llama10.png" data-sub-html="<h2>在 webui 中的第二行，可以选择</h2><p>webui</p>">
        
    </a><figcaption class="image-caption">在 webui 中的第二行，可以选择</figcaption>
    </figure></p>
<h3 id="31-adamw-优化器">3.1 AdamW 优化器</h3>
<p>是一种 Adam 优化器的改进形式，一种自动分析 loss 并且调整优化学习率的优化器，分析可以看<a href="https://www.jiqizhixin.com/articles/2018-07-03-14" target="_blank" rel="noopener noreffer ">机器之心的这篇文章</a>。学习的过程是在给定的一个初始点上计算损失函数的梯度，因为梯度的方向就是增加最快的方向，所以我们的点应该向梯度的反方向滑动一个距离（称为步长），然后再在这个点通过一些聪明的方法计算我们接下来要前往的方向，从而找到损失函数值很低的一个点。 AdamW 优化器就是一个非常聪明的方法。</p>
<p>最大梯度范数是在一些情况下，会出现梯度爆炸等不符合我们预期的情况，为了避免这些事情，我们有根据地限定梯度的范围，超出这个范围的就用范围的边界来表示。</p>
<p>训练精度上，在浮点数处理方面，计算机一般使用的 float 数据类型（称为 FP32）也就是单精度浮点数，根据 IEEE 标准，阶码是 8 位，尾数是 23 位。而半精度浮点数有两种类型（BF16 和 FP16），BF16 用 8 位表示阶码，7 位表示尾数；FP16 用 5 位表示阶码，10位表示尾数，在训练之前，注意<strong>你的 GPU 是否支持 BF16 数据类型</strong></p>
<p>学习率调节器一栏内容太多，就不在这里讲了。
对于 Evaluate&amp;Predict 一栏，有问题的可以看到<a href="https://www.ibm.com/docs/zh/watsonx/saas?topic=lab-model-parameters-prompting" target="_blank" rel="noopener noreffer ">IBM百科中的有关讲解</a></p>
]]></description>
</item>
<item>
    <title>使用LLaMA Factory训练模型指北（一、环境安装）</title>
    <link>http://croakero.github.io/posts/%E4%BD%BF%E7%94%A8llama-factory%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%8C%87%E5%8C%97%E4%B8%80%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/</link>
    <pubDate>Tue, 16 Jul 2024 09:15:11 &#43;0800</pubDate>
    <author>croakero</author>
    <guid>http://croakero.github.io/posts/%E4%BD%BF%E7%94%A8llama-factory%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%8C%87%E5%8C%97%E4%B8%80%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/</guid>
    <description><![CDATA[<p>本文适合于没有机器学习前置知识，但具有一定微积分和线性代数理解的读者，如果没有相应知识，文章中会给出稍许补充。</p>
<h2 id="1准备工作">1.准备工作</h2>
<ul>
<li>支持 CUDA 的 GPU ，如果没有，可以自行联系远程服务器</li>
<li>Linux bash 环境，如果没有，可以使用虚拟机构建</li>
<li>能够连接到 <a href="https://huggingface.co/" target="_blank" rel="noopener noreffer ">https://huggingface.co/</a> 的网络，用于下载模型,如果没有，需要用其他方法下载。能够连接到 <a href="https://github.com" target="_blank" rel="noopener noreffer ">https://github.com</a> 的网络，用于下载 github 仓库，如果没有，找个会的人教你，要么就去网上搜</li>
<li>一些基础的 linux 操作方法，文章中会提及部分</li>
<li>本文基于 Ubuntu22.04LTS</li>
</ul>
<h2 id="2使用-anaconda-管理包">2.使用 Anaconda 管理包</h2>
<h3 id="21-安装-anaconda">2.1 安装 Anaconda</h3>
<p>Anaconda 环境相较于传统的 pip 包管理来说，可以方便快捷地切换不同环境，如果把 pip 比作把所有的文件都摊到桌面上的笨手笨脚管理员， Anaconda 就像把文件归类放进不同文件夹里的爱干净管理员，这样的文件夹在 Anaconda 中称为“环境”。运行不同的程序所需的依赖不同，有时候会出现冲突，同时需要某个包的不同版本，就可以通过设置不同环境，基于某个环境运行不同程序。</p>
<p>安装 Anaconda 依赖代码如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>apt-get install libgl1-mesa-glx libegl1-mesa libxrandr2 libxrandr2 libxss1 libxcursor1 libxcomposite1 libasound2 libxi6 libxtst6
</span></span></code></pre></div><p>安装 Anaconda 安装器</p>
<pre tabindex="0"><code># 先运行这行
curl -O https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh
# 接着是这行
bash ~/Downloads/Anaconda3-2023.09-0-Linux-x86_64.sh
</code></pre><p>你也可以在<a href="https://www.runoob.com/python-qt/anaconda-tutorial.html" target="_blank" rel="noopener noreffer ">这里</a>看菜鸟教程给出的不同平台详细的安装方法，也可以去<a href="https://docs.anaconda.com/anaconda/install/linux/" target="_blank" rel="noopener noreffer ">这里</a>看官网给出的安装方法。</p>
<h3 id="22-创建-anaconda-环境安装-cuda-和-pytorch">2.2 创建 Anaconda 环境,安装 CUDA 和 PyTorch</h3>
<p>用 Anaconda 新建一个名为 LLaMA 的环境，使用 python3.11</p>
<pre tabindex="0"><code>conda create -n LLaMA python=3.11
# 中间会询问你是否 proceed（继续），输入y然后回车即可
</code></pre><p>激活这个环境，注意：<strong>在每次重新登陆的时候，必须输入以下命令激活这个环境，因为你在这个环境中安装了依赖项，否则无法找到依赖项</strong></p>
<pre tabindex="0"><code>conda activate LLaMA
# 成功后应该在命令行的用户前显示&#39;(LLaMA)&#39;字样
</code></pre><figure><a class="lightgallery" href="../images/llama1.png" title="../images/llama1.png" data-thumbnail="../images/llama1.png" data-sub-html="<h2>LLaMa字样</h2>">
        
    </a><figcaption class="image-caption">LLaMa字样</figcaption>
    </figure>
<p>然后我们来安装 CUDA，但是，<strong>什么是 CUDA 呢?</strong>
CUDA 是 NVIDIA 推出的一种软硬件集成技术，我们都知道显卡通常用于进行图像处理，比如电脑上运行的大型3D游戏，它的流畅程度和帧数都很吃 GPU 的水平，GPU 的特点是可以处理大型并法运算，而这种优势和在机器学习等领域非常有用。CUDA 能让我们利用这种 GPU 的这种特点来加速计算,但是 AMD 的显卡不支持这种技术，所以你需要 NVIDIA 的GPU。</p>
<p>接下来我们来安装 CUDA ，首先查看你的显卡支持的 CUDA 版本，在 bash 命令行输入这行代码</p>
<pre tabindex="0"><code>nvidia-smi
</code></pre><p><figure><a class="lightgallery" href="../images/llama2.png" title="nvidia-smi" data-thumbnail="../images/llama2.png" data-sub-html="<h2>输出后产生的界面，CUDA 版本在右上角</h2><p>nvidia-smi</p>">
        
    </a><figcaption class="image-caption">输出后产生的界面，CUDA 版本在右上角</figcaption>
    </figure></p>
<p>当前的 CUDA 版本，显示在表格最上面的一行。直接前往<a href="https://developer.nvidia.cn/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=22.04" target="_blank" rel="noopener noreffer ">NVIDIA官网</a>选择对应操作系统、架构等下载对应 CUDA 就好。</p>
<p><figure><a class="lightgallery" href="../images/llama3.png" title="CUDA" data-thumbnail="../images/llama3.png" data-sub-html="<h2>选择正确的 CUDA 版本</h2><p>CUDA</p>">
        
    </a><figcaption class="image-caption">选择正确的 CUDA 版本</figcaption>
    </figure></p>
<p>在安装 PyTorch 之前先简单介绍一下，它是一个 Python 程序库，有助于构建深度学习项目。它非常灵活，并允许我们使用深度学习领域惯用的 Python 来表示深度学习模型。它非常便捷，在刚刚创立的时候就吸引了许多使用者，并且在第 1 次发布之后的几年里，它已经成为应用程序中使用最广泛的深度学习工具之一，另一个比较出名的工具是 Tensorflow，但是更多被应用于生产中，在这里我们不作介绍。</p>
<p>直接来到<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreffer ">PyTorch官网</a>选择正确版本并下载。</p>
<p><figure><a class="lightgallery" href="../images/llama4.png" title="PyTorch" data-thumbnail="../images/llama4.png" data-sub-html="<h2>选择正确的PyTorch版本</h2><p>PyTorch</p>">
        
    </a><figcaption class="image-caption">选择正确的PyTorch版本</figcaption>
    </figure>
比如我想要下载基于 linux，Pip 方法下载 Python 语言，CUDA11.8的就需要运行这行命令</p>
<pre tabindex="0"><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
</code></pre><h2 id="3安装-llama-factory">3.安装 LLaMA-Factory</h2>
<h3 id="31-下载llama-facroty仓库">3.1 下载LLaMA-Facroty仓库</h3>
<p>为什么选择 LLaMA-Factory ? 因为它支持目前多种模型，集成了许多方法，支持多种精度，并且有非常易用的 webui 界面，对新手非常友好。在 github 上<a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md" target="_blank" rel="noopener noreffer ">项目地址</a></p>
<p>运行以下代码来安装LLaMA-Factory</p>
<pre tabindex="0"><code>git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &#34;.[torch,metrics]&#34;
</code></pre><p>可选的额外依赖项：torch、torch-npu、metrics、deepspeed、bitsandbytes、hqq、eetq、gptq、awq、aqlm、vllm、galore、badam、qwen、modelscope、quality</p>
<h3 id="32-通过远程机器访问-webui">3.2 通过远程机器访问 webui</h3>
<p>在本地，我们使用这行代码</p>
<pre tabindex="0"><code>llamafactory-cli webui
</code></pre><p><figure><a class="lightgallery" href="../images/llama5.png" title="webui" data-thumbnail="../images/llama5.png" data-sub-html="<h2>打开的 webui </h2><p>webui</p>">
        
    </a><figcaption class="image-caption">打开的 webui </figcaption>
    </figure>
启动 webui ，在命令行界面找到对应的网址访问即可（以0.0.0.0开头）但是这无法通过外部网络访问，只能在本机进行操作。现在我们通过远程机器访问它的 webui ，需要修改 launch() 文件。<br>
首先找到<code>LLaMA-Factory/src/llamafactory/webui</code>这个目录，打开<code>interface.py</code>这个文件，将 run_web_ui() 和 run_web_demo() 函数中各自最后一行的<code>launch()</code>函数修改成这样</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_web_ui</span>() <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>    gradio_share <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;GRADIO_SHARE&#34;</span>, <span style="color:#e6db74">&#34;0&#34;</span>)<span style="color:#f92672">.</span>lower() <span style="color:#f92672">in</span> [<span style="color:#e6db74">&#34;true&#34;</span>, <span style="color:#e6db74">&#34;1&#34;</span>]
</span></span><span style="display:flex;"><span>    server_name <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;GRADIO_SERVER_NAME&#34;</span>, <span style="color:#e6db74">&#34;0.0.0.0&#34;</span>)
</span></span><span style="display:flex;"><span>    create_ui()<span style="color:#f92672">.</span>queue()<span style="color:#f92672">.</span>launch(share<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, server_name<span style="color:#f92672">=</span>server_name, inbrowser<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,server_port<span style="color:#f92672">=</span><span style="color:#ae81ff">8887</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_web_demo</span>() <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>    gradio_share <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;GRADIO_SHARE&#34;</span>, <span style="color:#e6db74">&#34;0&#34;</span>)<span style="color:#f92672">.</span>lower() <span style="color:#f92672">in</span> [<span style="color:#e6db74">&#34;true&#34;</span>, <span style="color:#e6db74">&#34;1&#34;</span>]
</span></span><span style="display:flex;"><span>    server_name <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;GRADIO_SERVER_NAME&#34;</span>, <span style="color:#e6db74">&#34;0.0.0.0&#34;</span>)
</span></span><span style="display:flex;"><span>    client_loop: send disconnect: Broken pipeue, server_name<span style="color:#f92672">=</span>server_name, inbrowser<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,server_port<span style="color:#f92672">=</span><span style="color:#ae81ff">8887</span>)
</span></span></code></pre></div><p><strong>注意:</strong><code>share=True</code>表示允许共享，<code>server_port</code>规定了开启的端口，在这里规定为8887,接着就可以通过远程界面访问 webui 了，记得将0.0.0.0替换成远程主机的ip地址，如果无法连接，查看你远程主机的防火墙配置。</p>
<h3 id="33-通过-tmux-这一终端复用器解决-ssh-连接的问题">3.3 通过 tmux 这一终端复用器解决 ssh 连接的问题</h3>
<p>当你通过 ssh 连接远程主机运行某种任务的时候，可能会出现一些让人非常红温的问题： ssh 为了保证安全性，若几分钟没有操作就会断开连接，这样一来我们跑在远程主机上的任务就莫名其妙结束了，而且我们也不太想为了一个耗费很长时间的学习任务一直开着我们的电脑，或者一直开着我们的 ssh 连接，如果中间网莫名其妙断了几秒，我肯定相当沮丧，还得重新开始这个任务，白白浪费时间，我们引入 tmux 来解决这个问题。</p>
<p>像 tmux 这类的终端多路复用器可以允许我们基于面板和标签分割出多个终端窗口，这样便可以同时与多个 shell 会话进行交互，终端多路复用使我们可以分离当前终端会话并在将来重新连接。让你的远程工作非常舒服，避免了 nohup 和其他类似技巧的使用。</p>
<p>tmux 的安装命令</p>
<pre tabindex="0"><code>sudo apt-get install tmux
</code></pre><p>启动tmux就直接在终端执行<code>tmux</code>就可以。</p>
<p><figure><a class="lightgallery" href="../images/llama6.png" title="tmux" data-thumbnail="../images/llama6.png" data-sub-html="<h2>tmux界面示例</h2><p>tmux</p>">
        
    </a><figcaption class="image-caption">tmux界面示例</figcaption>
    </figure></p>
<p>在这里我只介绍一些简单的tmux命令，你可以在<a href="https://www.ruanyifeng.com/blog/2019/10/tmux.html" target="_blank" rel="noopener noreffer ">这里</a>看到比较详细的操作，还可以看<a href="https://missing-semester-cn.github.io/2020/command-line/" target="_blank" rel="noopener noreffer ">这个</a>和它配套的视频学习其他的有关计算机操作的相关知识。</p>
<p>在ssh连接成功后，键入<code>tmux</code>你会看到一个长得和终端差不多的界面，你可以在里面干任何你正常连接ssh之后做的事情，然后，如果你想把某个操作放在后台运行，键入<code>Ctrl+b</code>，再键入<code>d</code>，你就能看到你原来的ssh连接刚刚连上的样子，原来的窗口去哪里了呢？键入<code>tmux ls</code>指令查看目前所有窗口，第一个窗口一般表示为 0 ，输入<code>tmux attach -t 0</code>来重新进入后台的窗口，立即开始你的工作。</p>
<p>它的好处是你可以不用管ssh连接状态，任务会在 tmux 后台窗口里继续运行，你想干别的什么都可以，你还可以创建更多的 tmux 窗口，进行工作，然后退出ssh连接，无论你什么时候重新连接，那些窗口都保留在你退出后继续运行的样子，你可以快速恢复工作流。</p>
]]></description>
</item>
<item>
    <title>开博纪念</title>
    <link>http://croakero.github.io/posts/%E5%BC%80%E5%8D%9A%E7%BA%AA%E5%BF%B5/</link>
    <pubDate>Wed, 26 Apr 2023 20:02:32 &#43;0800</pubDate>
    <author>croakero</author>
    <guid>http://croakero.github.io/posts/%E5%BC%80%E5%8D%9A%E7%BA%AA%E5%BF%B5/</guid>
    <description><![CDATA[<h3 id="写在最前面">写在最前面</h3>
<p>在很早之前就有过建立博客的念头，但是看到了各种各样的未经许可的转载、洗稿等等这样那样的荒诞之事，常常把富有激情的创作者搞得焦头烂额。我用词往往词不达意，句子组织也形散神散，这样的文章，我想大概是不会被那群慧眼识珠的爱书人使用的。</p>
<p>这些年来，我的表达和写作欲望一直在降低，那么趁着我还没到一句话都不想说的时候，留下一些记忆吧。</p>
<h3 id="准则">准则</h3>
<p>不谈国是，不评时政，不写新闻</p>
]]></description>
</item>
</channel>
</rss>
